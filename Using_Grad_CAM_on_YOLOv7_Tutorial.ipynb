{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Grad-CAM YOLOv7 Demo\n",
        "This notebook demonstrates how to use Grad-CAM to visualize class activations in the YOLOv7 model.\n",
        "\n",
        "Note that to use it you will need a trained YOLOv7 model with the weights (.pt) And configurations (.yaml) and lastly the model code itself (yolov7.py).\n",
        "\n",
        "For easier use you can clone both repositories to google drive or feel free to copy paste my codes and adjust the original repos if it's easier for you or if you aren't working on google colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "xuBDOmLPdBSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount to your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmToyBjhdC6v",
        "outputId": "05a393ed-c926-4dc2-cf50-ac676136dcaf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clone the repos into your google drive\n",
        "!git clone https://github.com/azizjadehs/YOLOv7-to-be-used-on-Grad-CAM /content/gdrive/MyDrive/"
      ],
      "metadata": {
        "id": "ned1ksP_duAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "! pip install grad-cam\n",
        "! pip install torch torchvision"
      ],
      "metadata": {
        "id": "s461Y2WgdFDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from torchvision.models import resnet50\n",
        "from torchvision import transforms\n",
        "import yaml\n",
        "\n",
        "\n",
        "sys.path.append('')  # Add the clone path\n",
        "from src.classifier_output_target import ClassifierOutputTarget\n",
        "from models.yolo import Model, IDetect\n",
        "\n",
        "# Load the model configuration from YAML\n",
        "yaml_path = '.yaml' #this you can find in the original yolov7 data inside the cfg file exactley training/yolov7.yaml\n",
        "try:\n",
        "    with open(yaml_path, 'r') as f:\n",
        "        model_config = yaml.safe_load(f)\n",
        "    print(\"Model configuration loaded successfully\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {yaml_path} was not found.\")\n",
        "except yaml.YAMLError as e:\n",
        "    print(f\"Error loading YAML file: {e}\")\n",
        "\n",
        "# Initialize the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(model_config).to(device)\n",
        "\n",
        "\n",
        "# Load the trained weights from the .pt file\n",
        "weights_path = '.pt' #add the wanted weight after training cycle usually inside the runs/train\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(weights_path, map_location=device)\n",
        "    print(\"Weights loaded successfully\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The weights file {weights_path} was not found.\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error loading weights: {e}\")\n",
        "\n",
        "if 'model' in checkpoint:\n",
        "    state_dict = checkpoint['model'].state_dict()\n",
        "    print(\"Model is in checkpoint and state_dict loaded\")\n",
        "else:\n",
        "    state_dict = checkpoint\n",
        "    print(\"Model is not in checkpoint, using checkpoint directly as state_dict\")\n",
        "model.load_state_dict(state_dict)\n",
        "print(\"Model state_dict loaded successfully\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Choose the selected layer (I already found `104.rbr_dense.0 to be the best layer for heatmaps `)\n",
        "selected_layer = None\n",
        "for name, layer in model.model.named_modules():\n",
        "    if name == \"104.rbr_dense.0\":  # change the name as you want based on your layer name.\n",
        "        print(f\"Selected layer for Grad-CAM: {name}\")\n",
        "        selected_layer = layer\n",
        "        break\n",
        "\n",
        "if selected_layer is None:\n",
        "    raise ValueError(\"The selected layer was not found in the model.\")\n",
        "\n",
        "# Initialize Grad-CAM with the selected layer\n",
        "cam = GradCAM(model=model, target_layers=[selected_layer])\n",
        "#cam = GradCAMPlusPlus(model=model, target_layers=[selected_layer])\n",
        "\n",
        "\n",
        "# Specify the input folder containing images\n",
        "input_folder = ''  # Change this to your input folder's path\n",
        "\n",
        "# Specify the output folder where the Grad-CAM images will be saved-\n",
        "output_folder = '' #You can either view the result directly or better save them into a folder like the case of this tutorial\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "\n",
        "# Load and preprocess your image\n",
        "image_paths = [os.path.join(input_folder, fname) for fname in os.listdir(input_folder) if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
        "# Process each image in the folder\n",
        "for i, image_path in enumerate(image_paths):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((416, 416)),  # Resize to match your model's expected input\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #Standard for Imagenet\n",
        "    ])\n",
        "    input_image = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Perform a forward pass to get the model output\n",
        "    outputs = model(input_image)\n",
        "\n",
        "\n",
        "# Define the target class index\n",
        "    target_class_idx = 0  # Replace with the desired class index from your dataset\n",
        "    targets = [ClassifierOutputTarget(target_class_idx)]\n",
        "\n",
        "# Generate the Grad-CAM for the first target\n",
        "    grayscale_cam = cam(input_tensor=input_image, targets=targets)[0]\n",
        "    print(grayscale_cam)\n",
        "    # Convert image for visualization (undo normalization for display)\n",
        "    rgb_img = np.array(image)\n",
        "    rgb_img = np.float32(rgb_img) / 255  # Convert to float32 for visualization\n",
        "\n",
        "    # Resize the Grad-CAM mask to match the input image size\n",
        "    grayscale_cam_resized = cv2.resize(grayscale_cam, (rgb_img.shape[1], rgb_img.shape[0]))\n",
        "\n",
        "    # Convert the grayscale CAM to 3D (RGB)\n",
        "    grayscale_cam_resized = np.stack([grayscale_cam_resized] * 3, axis=-1)\n",
        "\n",
        "    # Visualize the result\n",
        "    visualization = show_cam_on_image(rgb_img, grayscale_cam_resized, use_rgb=True, image_weight=0.7)\n",
        "\n",
        "    # Ensure the visualization is scaled between 0 and 255\n",
        "    if visualization.max() > 1.0:\n",
        "        visualization = np.uint8(visualization)\n",
        "    else:\n",
        "        visualization = np.uint8(visualization * 255)\n",
        "\n",
        "    # Save the visualization image with a unique name\n",
        "    base_name = Path(image_path).stem  # Get the image file name without extension\n",
        "    save_path = os.path.join(output_folder, f'{base_name}_grad_cam_{i}.png')\n",
        "    Image.fromarray(visualization).save(save_path)\n",
        "    print(f\"Grad-CAM visualization saved as '{save_path}'\")"
      ],
      "metadata": {
        "id": "VXDu1RINdG5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using the script\n",
        "You can also run the script down, where you have only to put your arguments."
      ],
      "metadata": {
        "id": "N7Hq5Izit9p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter the cloned repos directory before running the script\n",
        "%cd /content/gdrive/MyDrive/"
      ],
      "metadata": {
        "id": "_oH-dfHuwX_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/main_script.py --yaml-path /content/gdrive/MyDrive/master/cfg/training/yolov7.yaml --weights-path /content/gdrive/MyDrive/yolov7/runs/train/exp45/weights/best.pt --input-folder /content/gdrive/MyDrive/master/mikroskop --output-folder /content/gdrive/MyDrive/TryingTheYOLO7Script --target-class-idx 2 --resize-dim 416 416 --selected-layer 104.rbr_dense.0\n"
      ],
      "metadata": {
        "id": "1_49W-ebuFtQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}